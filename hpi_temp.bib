@article{Sharma2023-br,
  title         = {Investigating Agency of {LLMs} in human-{AI} collaboration
                   tasks},
  author        = {Sharma, Ashish and Rao, Sudha and Brockett, Chris and
                   Malhotra, Akanksha and Jojic, Nebojsa and Dolan, Bill},
  journal       = {arXiv [cs.CL]},
  abstract      = {Agency, the capacity to proactively shape events, is central
                   to how humans interact and collaborate. While LLMs are being
                   developed to simulate human behavior and serve as human-like
                   agents, little attention has been given to the Agency that
                   these models should possess in order to proactively manage
                   the direction of interaction and collaboration. In this
                   paper, we investigate Agency as a desirable function of LLMs,
                   and how it can be measured and managed. We build on
                   social-cognitive theory to develop a framework of features
                   through which Agency is expressed in dialogue - indicating
                   what you intend to do (Intentionality), motivating your
                   intentions (Motivation), having self-belief in intentions
                   (Self-Efficacy), and being able to self-adjust
                   (Self-Regulation). We collect a new dataset of 83 human-human
                   collaborative interior design conversations containing 908
                   conversational snippets annotated for Agency features. Using
                   this dataset, we develop methods for measuring Agency of
                   LLMs. Automatic and human evaluations show that models that
                   manifest features associated with high Intentionality,
                   Motivation, Self-Efficacy, and Self-Regulation are more
                   likely to be perceived as strongly agentive.},
  month         = may,
  year          = 2023,
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@article{Hao2024-ak,
  title         = {Outlining the borders for {LLM} applications in patient
                   education: Developing an expert-in-the-loop {LLM}-powered
                   chatbot for prostate cancer patient education},
  author        = {Hao, Yuexing and Holmes, Jason and Waddle, Mark and Yu,
                   Nathan and Vickers, Kirstin and Preston, Heather and
                   Margolin, Drew and LÃ¶ckenhoff, Corinna E and Vashistha,
                   Aditya and Ghassemi, Marzyeh and Kalantari, Saleh and Liu,
                   Wei},
  journal       = {arXiv [cs.HC]},
  abstract      = {Cancer patients often struggle to transition swiftly to
                   treatment due to limited institutional resources, lack of
                   sophisticated professional guidance, and low health literacy.
                   The emergence of Large Language Models (LLMs) offers new
                   opportunities for such patients to access the wealth of
                   existing patient education materials. The current paper
                   presents the development process for an LLM-based chatbot
                   focused on prostate cancer education, including needs
                   assessment, co-design, and usability studies. The resulting
                   application, MedEduChat, integrates with patients' electronic
                   health record data and features a closed-domain,
                   semi-structured, patient-centered approach to address
                   real-world needs. This paper contributes to the growing field
                   of patient-LLM interaction by demonstrating the potential of
                   LLM-based chatbots to enhance prostate cancer patient
                   education and by offering co-design guidelines for future
                   LLM-based healthcare downstream applications.},
  month         = sep,
  year          = 2024,
  archiveprefix = {arXiv},
  primaryclass  = {cs.HC}
}

@article{Wu2024-kb,
  title         = {Reuse your rewards: Reward model transfer for zero-shot
                   cross-lingual alignment},
  author        = {Wu, Zhaofeng and Balashankar, Ananth and Kim, Yoon and
                   Eisenstein, Jacob and Beirami, Ahmad},
  journal       = {arXiv [cs.CL]},
  abstract      = {Aligning language models (LMs) based on human-annotated
                   preference data is a crucial step in obtaining practical and
                   performant LM-based systems. However, multilingual human
                   preference data are difficult to obtain at scale, making it
                   challenging to extend this framework to diverse languages. In
                   this work, we evaluate a simple approach for zero-shot
                   cross-lingual alignment, where a reward model is trained on
                   preference data in one source language and directly applied
                   to other target languages. On summarization and open-ended
                   dialog generation, we show that this method is consistently
                   successful under comprehensive evaluation settings, including
                   human evaluation: cross-lingually aligned models are
                   preferred by humans over unaligned models on up to >70\% of
                   evaluation instances. We moreover find that a
                   different-language reward model sometimes yields better
                   aligned models than a same-language reward model. We also
                   identify best practices when there is no language-specific
                   data for even supervised finetuning, another component in
                   alignment.},
  month         = apr,
  year          = 2024,
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@article{Morris2018-cs,
  title    = {Towards an artificially empathic conversational agent for mental
              health applications: System design and user perceptions},
  author   = {Morris, Robert R and Kouddous, Kareem and Kshirsagar, Rohan and
              Schueller, Stephen M},
  journal  = {J. Med. Internet Res.},
  volume   = 20,
  number   = 6,
  pages    = {e10148},
  abstract = {BACKGROUND: Conversational agents cannot yet express empathy in
              nuanced ways that account for the unique circumstances of the
              user. Agents that possess this faculty could be used to enhance
              digital mental health interventions. OBJECTIVE: We sought to
              design a conversational agent that could express empathic support
              in ways that might approach, or even match, human capabilities.
              Another aim was to assess how users might appraise such a system.
              METHODS: Our system used a corpus-based approach to simulate
              expressed empathy. Responses from an existing pool of online peer
              support data were repurposed by the agent and presented to the
              user. Information retrieval techniques and word embeddings were
              used to select historical responses that best matched a user's
              concerns. We collected ratings from 37,169 users to evaluate the
              system. Additionally, we conducted a controlled experiment
              (N=1284) to test whether the alleged source of a response (human
              or machine) might change user perceptions. RESULTS: The majority
              of responses created by the agent (2986/3770, 79.20\%) were deemed
              acceptable by users. However, users significantly preferred the
              efforts of their peers (P<.001). This effect was maintained in a
              controlled study (P=.02), even when the only difference in
              responses was whether they were framed as coming from a human or a
              machine. CONCLUSIONS: Our system illustrates a novel way for
              machines to construct nuanced and personalized empathic
              utterances. However, the design had significant limitations and
              further research is needed to make this approach viable. Our
              controlled study suggests that even in ideal conditions, nonhuman
              agents may struggle to express empathy as well as humans. The
              ethical implications of empathic agents, as well as their
              potential iatrogenic effects, are also discussed.},
  month    = jun,
  year     = 2018,
  keywords = {conversational agents; crowdsourcing; empathy; mental health; peer
              support},
  language = {en}
}

@article{Reis2024-vl,
  title     = {Practical applications of large language models for health care
               professionals and scientists},
  author    = {Reis, Florian and Lenz, Christian and Gossen, Manfred and Volk,
               Hans-Dieter and Drzeniek, Norman Michael},
  journal   = {JMIR Med. Inform.},
  publisher = {JMIR Publications Inc.},
  volume    = 12,
  pages     = {e58478},
  abstract  = {Unlabelled: With the popularization of large language models
               (LLMs), strategies for their effective and safe usage in health
               care and research have become increasingly pertinent. Despite the
               growing interest and eagerness among health care professionals
               and scientists to exploit the potential of LLMs, initial attempts
               may yield suboptimal results due to a lack of user experience,
               thus complicating the integration of artificial intelligence (AI)
               tools into workplace routine. Focusing on scientists and health
               care professionals with limited LLM experience, this viewpoint
               article highlights and discusses 6 easy-to-implement use cases of
               practical relevance. These encompass customizing translations,
               refining text and extracting information, generating
               comprehensive overviews and specialized insights, compiling ideas
               into cohesive narratives, crafting personalized educational
               materials, and facilitating intellectual sparring. Additionally,
               we discuss general prompting strategies and precautions for the
               implementation of AI tools in biomedicine. Despite various
               hurdles and challenges, the integration of LLMs into daily
               routines of physicians and researchers promises heightened
               workplace productivity and efficiency.},
  month     = sep,
  year      = 2024,
  keywords  = {AI; LLM; applications; artificial intelligence; chatGPT; health
               care; healthcare; large language model; physicians; prompting;
               scientists},
  language  = {en}
}

@article{Carlbring2023-st,
  title     = {A new era in Internet interventions: The advent of Chat-{GPT} and
               {AI}-assisted therapist guidance},
  author    = {Carlbring, Per and Hadjistavropoulos, Heather and Kleiboer, Annet
               and Andersson, Gerhard},
  journal   = {Internet Interv.},
  publisher = {Elsevier B.V.},
  volume    = 32,
  pages     = 100621,
  month     = apr,
  year      = 2023,
  keywords  = {Artificial intelligence guided interventions; Client preferences;
               Conversational agents; Digital self-help programs; Internet-based
               treatments; Therapeutic alliance},
  language  = {en}
}

@inproceedings{Yang2023-by,
  title     = {Harnessing biomedical literature to calibrate cliniciansâ trust
               in {AI} decision support systems},
  author    = {Yang, Qian and Hao, Yuexing and Quan, Kexin and Yang, Stephen and
               Zhao, Yiran and Kuleshov, Volodymyr and Wang, Fei},
  booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in
               Computing Systems},
  publisher = {ACM},
  address   = {New York, NY, USA},
  volume    = 16,
  pages     = {1--14},
  month     = apr,
  year      = 2023
}

@article{Mannhardt2024-ph,
  title         = {Impact of Large Language Model Assistance on Patients Reading
                   Clinical Notes: A Mixed-Methods Study},
  author        = {Mannhardt, Niklas and Bondi-Kelly, Elizabeth and Lam, Barbara
                   and O'Connell, Chloe and Asiedu, Mercy and Mozannar, Hussein
                   and Agrawal, Monica and Buendia, Alejandro and Urman, Tatiana
                   and Riaz, Irbaz B and Ricciardi, Catherine E and Ghassemi,
                   Marzyeh and Sontag, David},
  journal       = {arXiv [cs.HC]},
  abstract      = {Patients derive numerous benefits from reading their clinical
                   notes, including an increased sense of control over their
                   health and improved understanding of their care plan.
                   However, complex medical concepts and jargon within clinical
                   notes hinder patient comprehension and may lead to anxiety.
                   We developed a patient-facing tool to make clinical notes
                   more readable, leveraging large language models (LLMs) to
                   simplify, extract information from, and add context to notes.
                   We prompt engineered GPT-4 to perform these augmentation
                   tasks on real clinical notes donated by breast cancer
                   survivors and synthetic notes generated by a clinician, a
                   total of 12 notes with 3868 words. In June 2023, 200
                   female-identifying US-based participants were randomly
                   assigned three clinical notes with varying levels of
                   augmentations using our tool. Participants answered questions
                   about each note, evaluating their understanding of follow-up
                   actions and self-reported confidence. We found that
                   augmentations were associated with a significant increase in
                   action understanding score (0.63 $\pm$ 0.04 for select
                   augmentations, compared to 0.54 $\pm$ 0.02 for the control)
                   with p=0.002. In-depth interviews of self-identifying breast
                   cancer patients (N=7) were also conducted via video
                   conferencing. Augmentations, especially definitions, elicited
                   positive responses among the seven participants, with some
                   concerns about relying on LLMs. Augmentations were evaluated
                   for errors by clinicians, and we found misleading errors
                   occur, with errors more common in real donated notes than
                   synthetic notes, illustrating the importance of carefully
                   written clinical notes. Augmentations improve some but not
                   all readability metrics. This work demonstrates the potential
                   of LLMs to improve patients' experience with clinical notes
                   at a lower burden to clinicians. However, having a human in
                   the loop is important to correct potential model errors.},
  month         = jan,
  year          = 2024,
  archiveprefix = {arXiv},
  primaryclass  = {cs.HC}
}

@inproceedings{Sharma2021-qo,
  title     = {Towards Facilitating Empathic Conversations in Online Mental
               Health Support: A Reinforcement Learning Approach},
  author    = {Sharma, Ashish and Lin, Inna W and Miner, Adam S and Atkins,
               David C and Althoff, Tim},
  booktitle = {Proceedings of the Web Conference 2021},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  pages     = {194--205},
  abstract  = {Online peer-to-peer support platforms enable conversations
               between millions of people who seek and provide mental health
               support. If successful, web-based mental health conversations
               could improve access to treatment and reduce the global disease
               burden. Psychologists have repeatedly demonstrated that empathy,
               the ability to understand and feel the emotions and experiences
               of others, is a key component leading to positive outcomes in
               supportive conversations. However, recent studies have shown that
               highly empathic conversations are rare in online mental health
               platforms. In this paper, we work towards improving empathy in
               online mental health support conversations. We introduce a new
               task of empathic rewriting which aims to transform low-empathy
               conversational posts to higher empathy. Learning such
               transformations is challenging and requires a deep understanding
               of empathy while maintaining conversation quality through text
               fluency and specificity to the conversational context. Here we
               propose Partner, a deep reinforcement learning (RL) agent that
               learns to make sentence-level edits to posts in order to increase
               the expressed level of empathy while maintaining conversation
               quality. Our RL agent leverages a policy network, based on a
               transformer language model adapted from GPT-2, which performs the
               dual task of generating candidate empathic sentences and adding
               those sentences at appropriate positions. During training, we
               reward transformations that increase empathy in posts while
               maintaining text fluency, context specificity, and diversity.
               Through a combination of automatic and human evaluation, we
               demonstrate that Partner successfully generates more empathic,
               specific, and diverse responses and outperforms NLP methods from
               related tasks such as style transfer and empathic dialogue
               generation. This work has direct implications for facilitating
               empathic conversations on web-based platforms.},
  series    = {WWW '21},
  month     = jun,
  year      = 2021
}

@article{Ghassemi2021-ob,
  title    = {The false hope of current approaches to explainable artificial
              intelligence in health care},
  author   = {Ghassemi, Marzyeh and Oakden-Rayner, Luke and Beam, Andrew L},
  journal  = {Lancet Digit Health},
  volume   = 3,
  number   = 11,
  pages    = {e745--e750},
  abstract = {The black-box nature of current artificial intelligence (AI) has
              caused some to question whether AI must be explainable to be used
              in high-stakes scenarios such as medicine. It has been argued that
              explainable AI will engender trust with the health-care workforce,
              provide transparency into the AI decision making process, and
              potentially mitigate various kinds of bias. In this Viewpoint, we
              argue that this argument represents a false hope for explainable
              AI and that current explainability methods are unlikely to achieve
              these goals for patient-level decision support. We provide an
              overview of current explainability techniques and highlight how
              various failure cases can cause problems for decision making for
              individual patients. In the absence of suitable explainability
              methods, we advocate for rigorous internal and external validation
              of AI models as a more direct means of achieving the goals often
              associated with explainability, and we caution against having
              explainability be a requirement for clinically deployed models.},
  month    = nov,
  year     = 2021,
  language = {en}
}

@article{Ghassemi2019-pu,
  title    = {Practical guidance on artificial intelligence for health-care data},
  author   = {Ghassemi, Marzyeh and Naumann, Tristan and Schulam, Peter and
              Beam, Andrew L and Chen, Irene Y and Ranganath, Rajesh},
  journal  = {Lancet Digit Health},
  volume   = 1,
  number   = 4,
  pages    = {e157--e159},
  month    = aug,
  year     = 2019,
  language = {en}
}

@article{Ghassemi2022-fy,
  title    = {Machine learning and health need better values},
  author   = {Ghassemi, Marzyeh and Mohamed, Shakir},
  journal  = {NPJ Digit Med},
  volume   = 5,
  number   = 1,
  pages    = 51,
  abstract = {Health care is a human process that generates data from human
              lives, as well as the care they receive. Machine learning has
              worked in health to bring new technology into this sociotechnical
              environment, using data to support a vision of healthier living
              for everyone. Interdisciplinary fields of research like machine
              learning for health bring different values and judgements
              together, requiring that those value choices be deliberate and
              measured. More than just abstract ideas, our values are the basis
              upon which we choose our research topics, set up research
              collaborations, execute our research methodologies, make
              assessments of scientific and technical correctness, proceed to
              product development, and finally operationalize deployments and
              describe policy. For machine learning to achieve its aims of
              supporting healthier living while minimizing harm, we believe that
              a deeper introspection of our fieldâs values and contentions is
              overdue. In this perspective, we highlight notable areas in need
              of attention within the field. We believe deliberate and informed
              introspection will lead our community to renewed opportunities for
              understanding disease, new partnerships with clinicians and
              patients, and allow us to better support people and communities to
              live healthier, dignified lives.},
  month    = apr,
  year     = 2022,
  language = {en}
}

@article{Ghassemi2023-mm,
  title   = {{ChatGPT} one year on: who is using it, how and why?},
  author  = {Ghassemi, Marzyeh and Birhane, Abeba and Bilal, Mushtaq and
             Kankaria, Siddharth and Malone, Claire and Mollick, Ethan and
             Tustumi, Francisco},
  journal = {Nature},
  volume  = 624,
  number  = 7990,
  pages   = {39--41},
  year    = 2023
}

@article{Ghassemi2023-nc,
  title    = {Presentation matters for {AI}-generated clinical advice},
  author   = {Ghassemi, Marzyeh},
  journal  = {Nature Human Behaviour},
  volume   = 7,
  number   = 11,
  pages    = {1833--1835},
  month    = nov,
  year     = 2023,
  language = {en}
}

@article{Johri_undated-yp,
  title  = {Guidelines For Rigorous Evaluation of Clinical {LLMs} For
            Conversational Reasoning},
  author = {Johri, Shreya and Jeong, Jaehwan and Tran, Benjamin A and
            Schlessinger, Daniel I and Wongvibulsin, Shannon and Cai, Zhuo Ran
            and Daneshjou, Roxana and Rajpurkar, Pranav}
}

@article{Sharma2023-qw,
  title     = {{HumanâAI} collaboration enables more empathic conversations in
               text-based peer-to-peer mental health support},
  author    = {Sharma, Ashish and Lin, Inna W and Miner, Adam S and Atkins,
               David C and Althoff, Tim},
  journal   = {Nature Machine Intelligence},
  publisher = {Nature Publishing Group},
  volume    = 5,
  number    = 1,
  pages     = {46--57},
  abstract  = {Advances in artificial intelligence (AI) are enabling systems
               that augment and collaborate with humans to perform simple,
               mechanistic tasks such as scheduling meetings and
               grammar-checking text. However, such humanâAI collaboration poses
               challenges for more complex tasks, such as carrying out empathic
               conversations, due to the difficulties that AI systems face in
               navigating complex human emotions and the open-ended nature of
               these tasks. Here we focus on peer-to-peer mental health support,
               a setting in which empathy is critical for success, and examine
               how AI can collaborate with humans to facilitate peer empathy
               during textual, online supportive conversations. We develop
               HAILEY, an AI-in-the-loop agent that provides just-in-time
               feedback to help participants who provide support (peer
               supporters) respond more empathically to those seeking help
               (support seekers). We evaluate HAILEY in a non-clinical
               randomized controlled trial with real-world peer supporters on
               TalkLife (N = 300), a large online peer-to-peer support platform.
               We show that our humanâAI collaboration approach leads to a
               19.6\% increase in conversational empathy between peers overall.
               Furthermore, we find a larger, 38.9\% increase in empathy within
               the subsample of peer supporters who self-identify as
               experiencing difficulty providing support. We systematically
               analyse the humanâAI collaboration patterns and find that peer
               supporters are able to use the AI feedback both directly and
               indirectly without becoming overly reliant on AI while reporting
               improved self-efficacy post-feedback. Our findings demonstrate
               the potential of feedback-driven, AI-in-the-loop writing systems
               to empower humans in open-ended, social and high-stakes tasks
               such as empathic conversations. AI language modelling and
               generation approaches have developed fast in the last decade,
               opening promising new directions in humanâAI collaboration. An
               AI-in-the loop conversational system called HAILEY is developed
               to empower peer supporters in providing empathic responses to
               mental health support seekers.},
  month     = jan,
  year      = 2023,
  language  = {en}
}

@article{Juhling_undated-fu,
  title  = {Uncovering Design Requirements for Successful {mHealth}
            Interventions in Chronic Pelvic Pain Patients Through Topic Modeling},
  author = {JÃ¼hling, Daniel}
}

@article{Singhal2023-st,
  title    = {Large language models encode clinical knowledge},
  author   = {Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S
              Sara and Wei, Jason and Chung, Hyung Won and Scales, Nathan and
              Tanwani, Ajay and Cole-Lewis, Heather and Pfohl, Stephen and
              Payne, Perry and Seneviratne, Martin and Gamble, Paul and Kelly,
              Chris and Babiker, Abubakr and SchÃ¤rli, Nathanael and Chowdhery,
              Aakanksha and Mansfield, Philip and Demner-Fushman, Dina and
              AgÃ¼era Y Arcas, Blaise and Webster, Dale and Corrado, Greg S and
              Matias, Yossi and Chou, Katherine and Gottweis, Juraj and Tomasev,
              Nenad and Liu, Yun and Rajkomar, Alvin and Barral, Joelle and
              Semturs, Christopher and Karthikesalingam, Alan and Natarajan,
              Vivek},
  journal  = {Nature},
  volume   = 620,
  number   = 7972,
  pages    = {172--180},
  abstract = {Large language models (LLMs) have demonstrated impressive
              capabilities, but the bar for clinical applications is high.
              Attempts to assess the clinical knowledge of models typically rely
              on automated evaluations based on limited benchmarks. Here, to
              address these limitations, we present MultiMedQA, a benchmark
              combining six existing medical question answering datasets
              spanning professional medicine, research and consumer queries and
              a new dataset of medical questions searched online,
              HealthSearchQA. We propose a human evaluation framework for model
              answers along multiple axes including factuality, comprehension,
              reasoning, possible harm and bias. In addition, we evaluate
              Pathways Language Model1 (PaLM, a 540-billion parameter LLM) and
              its instruction-tuned variant, Flan-PaLM2 on MultiMedQA. Using a
              combination of prompting strategies, Flan-PaLM achieves
              state-of-the-art accuracy on every MultiMedQA multiple-choice
              dataset (MedQA3, MedMCQA4, PubMedQA5 and Measuring Massive
              Multitask Language Understanding (MMLU) clinical topics6),
              including 67.6\% accuracy on MedQA (US Medical Licensing
              Exam-style questions), surpassing the prior state of the art by
              more than 17\%. However, human evaluation reveals key gaps. To
              resolve this, we introduce instruction prompt tuning, a
              parameter-efficient approach for aligning LLMs to new domains
              using a few exemplars. The resulting model, Med-PaLM, performs
              encouragingly, but remains inferior to clinicians. We show that
              comprehension, knowledge recall and reasoning improve with model
              scale and instruction prompt tuning, suggesting the potential
              utility of LLMs in medicine. Our human evaluations reveal
              limitations of today's models, reinforcing the importance of both
              evaluation frameworks and method development in creating safe,
              helpful LLMs for clinical applications.},
  month    = aug,
  year     = 2023,
  language = {en}
}

@article{Pfohl2024-da,
  title         = {A Toolbox for Surfacing Health Equity Harms and Biases in
                   Large Language Models},
  author        = {Pfohl, Stephen R and Cole-Lewis, Heather and Sayres, Rory and
                   Neal, Darlene and Asiedu, Mercy and Dieng, Awa and Tomasev,
                   Nenad and Rashid, Qazi Mamunur and Azizi, Shekoofeh and
                   Rostamzadeh, Negar and McCoy, Liam G and Celi, Leo Anthony
                   and Liu, Yun and Schaekermann, Mike and Walton, Alanna and
                   Parrish, Alicia and Nagpal, Chirag and Singh, Preeti and
                   Dewitt, Akeiylah and Mansfield, Philip and Prakash, Sushant
                   and Heller, Katherine and Karthikesalingam, Alan and Semturs,
                   Christopher and Barral, Joelle and Corrado, Greg and Matias,
                   Yossi and Smith-Loud, Jamila and Horn, Ivor and Singhal,
                   Karan},
  journal       = {arXiv [cs.CY]},
  abstract      = {Large language models (LLMs) hold immense promise to serve
                   complex health information needs but also have the potential
                   to introduce harm and exacerbate health disparities. Reliably
                   evaluating equity-related model failures is a critical step
                   toward developing systems that promote health equity. In this
                   work, we present resources and methodologies for surfacing
                   biases with potential to precipitate equity-related harms in
                   long-form, LLM-generated answers to medical questions and
                   then conduct an empirical case study with Med-PaLM 2,
                   resulting in the largest human evaluation study in this area
                   to date. Our contributions include a multifactorial framework
                   for human assessment of LLM-generated answers for biases, and
                   EquityMedQA, a collection of seven newly-released datasets
                   comprising both manually-curated and LLM-generated questions
                   enriched for adversarial queries. Both our human assessment
                   framework and dataset design process are grounded in an
                   iterative participatory approach and review of possible
                   biases in Med-PaLM 2 answers to adversarial queries. Through
                   our empirical study, we find that the use of a collection of
                   datasets curated through a variety of methodologies, coupled
                   with a thorough evaluation protocol that leverages multiple
                   assessment rubric designs and diverse rater groups, surfaces
                   biases that may be missed via narrower evaluation approaches.
                   Our experience underscores the importance of using diverse
                   assessment methodologies and involving raters of varying
                   backgrounds and expertise. We emphasize that while our
                   framework can identify specific forms of bias, it is not
                   sufficient to holistically assess whether the deployment of
                   an AI system promotes equitable health outcomes. We hope the
                   broader community leverages and builds on these tools and
                   methods towards realizing a shared goal of LLMs that promote
                   accessible and equitable healthcare for all.},
  month         = mar,
  year          = 2024,
  archiveprefix = {arXiv},
  primaryclass  = {cs.CY}
}

@article{Chen2021-zu,
  title    = {Ethical Machine Learning in Healthcare},
  author   = {Chen, Irene Y and Pierson, Emma and Rose, Sherri and Joshi,
              Shalmali and Ferryman, Kadija and Ghassemi, Marzyeh},
  journal  = {Annu Rev Biomed Data Sci},
  volume   = 4,
  pages    = {123--144},
  abstract = {The use of machine learning (ML) in healthcare raises numerous
              ethical concerns, especially as models can amplify existing health
              inequities. Here, we outline ethical considerations for equitable
              ML in the advancement of healthcare. Specifically, we frame ethics
              of ML in healthcare through the lens of social justice. We
              describe ongoing efforts and outline challenges in a proposed
              pipeline of ethical ML in health, ranging from problem selection
              to postdeployment considerations. We close by summarizing
              recommendations to address these challenges.},
  month    = jul,
  year     = 2021,
  keywords = {bias; ethics; health; health disparities; healthcare; machine
              learning},
  language = {en}
}

@article{Lago2015-xp,
  title     = {Framing sustainability as a property of software quality},
  author    = {Lago, Patricia and KoÃ§ak, Sedef Akinli and Crnkovic, Ivica and
               Penzenstadler, Birgit},
  journal   = {Commun. ACM},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  volume    = 58,
  number    = 10,
  pages     = {70--78},
  abstract  = {This framework addresses the environmental dimension of software
               performance, as applied here by a paper mill and a car-sharing
               service.},
  month     = sep,
  year      = 2015
}